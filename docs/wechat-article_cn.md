# 跟着莫欣老师用 VibeCoding + Agently，6 小时做出一个“能联调、能回归”的智能 ToDo（但 ToDo 不是重点）

项目仓库：github.com/AgentEra/Agently-NexusTodo  
Agently 框架：github.com/AgentEra/Agently

本文视角：采访复盘/运营观察，写给开发者、VC、关注 AI 应用的人，以及对“新开发范式”好奇的读者。

如果你最近刷到 “VibeCoding”，大概率会有一种冲动：把需求丢给模型，“一路 Accept All”，然后期待它自己把项目写完。

我在直播里看到，莫欣老师也做了类似的尝试。但关注点不在“写得快不快”，而是：**怎么把这种新范式做成可复制、可交付的工程流程**。

在 51CTO 的一场直播里（由 Agently 框架核心作者莫欣老师主讲），他们用 VibeCoding 的方式完成了一个端到端的智能 ToDo 系统：Go 后端 + 基于 Agently 的智能服务 + 支持流式输出的 Web 客户端。整体开发与优化约 6 小时，其中直播教学覆盖约 4 小时；Golang 后端由 TRAE 实现，其它部分由 VSCode + Codex 协作完成。

围绕“怎么把智能模块做成工程交付”这个命题，我问了 Agently 框架核心作者莫欣老师一个问题：

> Tips（提问）  
> 你最希望观众带走的“一个关键认知”是什么？

莫欣老师给了我三句话（我把表达做了适度整理，但意思不变）：

> Tips（回答要点）  
> 1）SpecDD 不是“形式主义”，它是并行开发的起点。  
> 2）API 文档规范不是“最后补说明书”，它是联调协作的基准。  
> 3）智能模块要能落地，不能靠“硬写”，要靠框架能力 + 可执行测试把不确定性收住。

> Tips（一句话带走）  
> “写出来”不难，难的是“跑起来、联起来、跑得稳”。

## 先把“架子”搭起来，再把“大脑”装上去

顺着“跑起来、联起来、跑得稳”这个标准，莫欣老师团队的做法也很朴素：先把业务系统该有的骨架搭起来，再把智能能力装上去。

NexusTodo 的整体架构很简单，可以理解成三块拼图：

1) **ToDo 后端（Go）**  
提供最基础、最稳定的一层：任务的增删改查 API、数据持久化、设备/用户标识等。它的目标是“像业务系统一样可靠”，让上层智能能力有一个可依赖的落点。

2) **Web 客户端（前端）**  
先把 UI 和交互体验跑起来：列表展示、聊天输入、流式输出的气泡、最终的任务卡片。它的目标是“用户能真实用起来”，而不是只在命令行里看日志。

3) **智能服务（Python，auto_agent，基于 Agently）**  
这是“装上去的大脑”：负责理解自然语言、多轮对话、工具调用（多次请求 ToDo 后端做查询/写入）、以及把过程以流式方式推给前端。它的目标是“把不确定性变成可控的工程流程”。

三者之间的关系也很直观：
- 后端是“事实来源”（数据以它为准）
- 智能服务是“决策与执行”（会多次调用后端，把复杂意图拆成可执行步骤）
- 前端负责“把过程讲清楚”（进行中输出 vs 最终结论清晰分层）

他们的开发顺序也符合这个结构：  
**先把 ToDo 场景的后端 + 客户端基础架子搭好**，让接口与体验都可跑通；然后再在这个基础上，通过 `auto_agent/` 把“智能能力”加进来，并用真实接口的场景用例一轮轮打磨到可用。

---

## 1) 51CTO 直播里，我最希望你带走的一个关键认知

**SpecDD 很重要。**

更具体一点：当你在做一个“多模块协同”的项目时（后端 / 前端 / 智能模块 / 测试脚本），API 文档规范不是“最后补一补的说明书”，而是贯穿全程的 **协作基准**：

- 你可以并行开发，而不是串行等人。
- 你可以写“可执行”的集成场景，而不是靠前端 UI 目测对不对。
- 你可以在联调时快速定位问题：到底是接口实现错、数据不一致、还是智能模块决策错。

> Tips（给读者的直觉）  
> 联调就是照妖镜：你写得再漂亮，一联调就知道“到底谁在胡说”。

---

## 2) VibeCoding 的真实形态：你更像在协调一个技术团队

> Tips（提问）  
> 在 VibeCoding 里，你到底在做什么？

莫欣老师的比喻很形象：与其说是“模型替你写代码”，不如说是**你在协调、指挥、管理一个技术团队**。每个独立会话就像一支小队：前端一队、后端一队、智能模块一队，测试也各自有自己的会话。

他复盘他们当时的组织方式，大概是这样：
- 前端一个会话：负责 UI、流式展示、任务卡片等。
- 后端一个会话：负责 REST API、持久化、设备注册等。
- 智能模块一个会话：负责语义理解、工具调用、ReAct（思考-行动-观察）循环、SSE（服务端推送）输出协议等。
- 各自配套的测试会话：端到端联调之前，先把局部可验证的部分压实。

更有意思的是“会话合并”：

早期他们对智能模块会话做了较严格的权限约束（只能在特定目录工作），这样可以逼迫它先把自己模块做好；  
当项目进入联调阶段，智能问题开始跨文件、跨模块出现，他们会逐步放开目录权限，直到它能看完整个项目——这个会话也就自然演化成“维护全仓”的会话。

这件事给我的启发是：**权限与上下文不是越大越好，而是应该跟着工程阶段动态调整。**

> Tips（类比一下更好懂）  
> 你可以把它想象成“项目拉群”：前期各自打磨，后期必须拉到一张桌子上对齐，才不会在联调阶段互相甩锅。

---

## 3) 智能模块的“低级坑”：模型会退回传统做法（直到你把它拉回来）

> Tips（提问）  
> 智能模块最容易踩的坑是什么？

一方面，Agently 的语料还不足以成为模型的“通识”；  
另一方面，面向智能的逻辑天然更难开发、测试、优化。于是模型会出现一些看起来“很离谱”的行为——比如在做语义理解和意图识别时，**第一反应是用分词、关键词匹配这类传统方案**，而不是用大模型（尤其是经过 Agently 做了结构化约束后，相对更稳定的能力）去解决。

> Tips（莫欣老师补充）  
> 这类问题不是“模型不聪明”，而是“你没把它拉回正确的解决方式”。

他们把这些踩坑记录沉淀进了仓库里的 skills 文档，里面甚至直接写了 “Lessons from NexusTodo”，你可以当成一份“下一次别再踩”的清单：
- github.com/AgentEra/Agently-NexusTodo/blob/main/auto_agent/docs/skills/agently-agent-systems/SKILL.md
- github.com/AgentEra/Agently-NexusTodo/blob/main/auto_agent/docs/skills/agently-agents-and-prompts/SKILL.md

一句话总结：**智能模块要用大模型做“智能事”，但前提是你要用工程化约束把它的能力锁在正确轨道上。**

---

## 4) 为什么是 Agently：把复杂度“从代码里挪走”

> Tips（提问）  
> 为什么不让模型把智能模块直接硬写到底？为什么要引入 Agently 这样的框架？

莫欣老师的回答很直接：如果没有框架，VibeCoding 很容易变成“prompt 玄学 + 解析地狱 + 线上返工”。而 Agently 在这次项目里，提供的是一组“把不确定性收住”的工程原语：

1) **不依赖接口参数的稳定结构输出**  
通过 Agently 的 Output Format（结构化输出格式）+ Ensure Keys（关键字段兜底），他们让智能服务输出“可校验的结构化结果”，避免“自然语言解释一大堆但关键字段缺失”的不稳定。

2) **简单可靠的信号驱动编排表达**  
通过 TriggerFlow（信号驱动编排），把“多轮对话 + 多次工具调用 + 条件分支”表达成更清晰的流程，而不是一坨 if-else。

3) **更可控的 ReAct 循环与流式交互**  
流式输出把“过程”和“结论”分开；同时限制最多 10 步，避免无限循环，并把最终任务卡片作为“结论的一部分”统一下发。

他说到这里，落点其实很清楚：智能模块不能只靠“硬写”，必须靠框架能力 + 文档规范 + 可执行测试，把它拉回工程世界。

---

## 5) 如果你让模型“硬写整个智能模块”，现实里会撞上哪些坑？

> Tips（提问）  
> 如果真的“硬写到底”，最常见的坑会是什么？

莫欣老师的判断是：很多坑并不“高级”，但会在真实项目里反复出现。这里我也结合一些公开讨论/研究里反复出现的风险（你也能看到很多同类案例在 X / Reddit / 技术媒体里出现）做个归纳：

1) **“Accept All” 会放大不可控性**  
“Vibe coding” 本身是一种很有感染力的表达（Karpathy 的原帖引发了大量讨论），但很多人也会在项目复杂度上来后发现：代码越写越多，你越来越不理解它，修 bug 变成“碰运气”。  
参考：Karpathy 原帖（X）与讨论（例如 Ars Technica 的综述文章）。

2) **质量回归：逻辑 bug、维护成本、评审尾巴都会上来**  
一些对 PR 的分析与报道指出：AI 参与代码生成后，工程实践很容易出现“看起来很快，但尾部成本更高”的情况（返工、review、缺陷修复）。  
参考：CodeRabbit 对开源 PR 的分析与媒体报道（例如 The Register 的报道）。

3) **安全与过度自信问题**  
研究发现，在 AI 辅助下写出的代码，往往更容易出现安全问题；同时开发者对自己产出的安全性更自信——这是非常危险的组合。  
参考：Stanford / Dan Boneh 团队相关研究的报道。

4) **供应链风险：幻觉依赖 + “slopsquatting”**  
当模型“编”出并不存在的包名时，攻击者可以抢注同名包并投毒，再诱导安装（这类供应链攻击路径并不新，但在 AI 时代更容易被触发）。  
参考：arXiv 上关于 “slopsquatting / package hallucination” 的研究，以及相关安全媒体的解读。

那 Agently 怎么帮开发团队绕开这些坑？

- 用 **SpecDD + API 文档规范** 把系统分层，先把协作基准定清楚。
- 用 **结构化输出** 把智能模块的输出变成可校验的数据，而不是一段“解释”。
- 用 **TriggerFlow/编排** 把多步骤任务写成“可读、可调、可测试”的流程。
- 用 **真实接口集成场景测试** 把“看起来能用”变成“真的能用”。
- 用 **权限/上下文渐进开放** 把联调复杂度收敛在可控范围。

---

## 6) 只要对 AI 应用开发有爱，谁来都欢迎

> Tips（提问）  
> 如果读者想入门、想上手，最重要的一句话是什么？

莫欣老师的回答很朴素：只要你对 AI 应用开发有热情，谁来都欢迎。更具体一点，如果你：
- 正在把大模型嵌进现有业务系统
- 想把多轮对话、工具调用、流式交互、复杂流程编排做得更稳
- 想找一套可复制的“智能体系统”开发范式

莫欣老师也欢迎你来 Agently 社区一起讨论、一起踩坑、一起把坑填平。

---

## 7) 加入 Agently 微信讨论群（官方入口）

你可以通过以下方式找到官方微信群入口：
- 访问 Agently 官网：Agently.tech
- 访问 Agently GitHub 主页：github.com/AgentEra/Agently

申请表链接（来自官方文档/README 当时的指向）：
- doc.weixin.qq.com/forms/AIoA8gcHAFMAScAhgZQABIlW6tV3l7QQf

---

## 8) 给你留一个“上手挑战”（也欢迎你一起）

> Tips（提问）  
> 能不能给大家留一个“上手挑战”？

莫欣老师说当然可以。于是我把挑战整理成一个可传播、可验证、也能真实提升工程能力的版本：

**Agently VibeCoding 上线挑战（48 小时 / 周末版）**

目标：把一个“带智能模块的功能”做成可演示、可复现、可回归的工程交付，而不仅仅是个 Demo。

最低交付标准（建议清单）：
1) 一份 SpecDD（1-2 页就够）：目标、边界、不可做的事、成功标准。
2) 一份 API 文档规范：接口、字段、状态码、示例请求响应。
3) 一个智能模块：多轮对话 + 至少 2 次工具调用（查找 + 决策 + 写操作）。
4) 一个流式 UI：能区分“进行中输出”与“最终结论”。
5) 至少 10 条真实接口集成用例：写成脚本，能一键跑通。
6) 一段 60 秒录屏：展示从自然语言到真实系统变更的全过程。

你把成果发到：
- Agently GitHub Discussions（可公开复盘）
- 或者发到微信群（适合快速交流与反馈）

这样也许就能把“VibeCoding”从一种氛围，变成一种真正可复用的工程能力。

---

## 相关阅读（公开资料）

- Karpathy X 原帖（vibe coding / accept all）：x.com/karpathy/status/1886192184808149383
- Reddit 讨论样本（vibe coding 体验与困惑）：reddit.com/r/ClaudeAI/comments/1igppfg/i_dont_know_how_to_vibe_code/
- Ars Technica 综述（vibe coding 讨论）：arstechnica.com/ai/2025/03/is-vibe-coding-with-ai-gnarly-or-reckless-maybe-some-of-both/
- The Register 报道（AI 代码缺陷/评审尾巴）：theregister.com/2025/12/17/ai_code_bugs/
- CodeRabbit 报告入口（新闻稿）：businesswire.com/news/home/20251217666881/en/CodeRabbits-State-of-AI-vs-Human-Code-Generation-Report-Finds-That-AI-Written-Code-Produces-1.7x-More-Issues-Than-Human-Code
- Stanford EE（Dan Boneh 团队：bug + 安全 + 过度自信）：ee.stanford.edu/dan-boneh-and-team-find-relying-ai-more-likely-make-your-code-buggier
- arXiv（package hallucination / slopsquatting 论文）：arxiv.org/abs/2406.10279
- Help Net Security（包名幻觉与 slopsquatting 报道）：helpnetsecurity.com/2025/04/14/package-hallucination-slopsquatting-malicious-code/
